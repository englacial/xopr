{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291a2191",
   "metadata": {},
   "source": [
    "---\n",
    "title: Loading bed picks -- Work in progress exploration\n",
    "date: 2025-11-25\n",
    "---\n",
    "\n",
    "This notebook is meant as a starting point for a conversation about how to handle OPR bed picks and how to integrate other bed pick sources (primarily the BedMap3 dataset) into xOPR.\n",
    "\n",
    "Right now, this is on a custom branch with some basic fixes to layer loading but not major changes to the structure. We may want to rethink bed picking more significantly, though.\n",
    "\n",
    "There are really two somewhat distinct bed pick workflows:\n",
    "1. Primarily working with bed picks and wanting to be able to trace back to the radar data when needed\n",
    "2. Primarily working with radar data and wanting to use bed picks as context into that data\n",
    "\n",
    "Serving both with the same interface may be a bit tricky, but I think it'll be worth the effort. Look at the `demo_notebook.ipynb` for context on how the second use case works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xopr\n",
    "\n",
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "import hvplot\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import geoviews.feature as gf\n",
    "import cartopy.crs as ccrs\n",
    "import rioxarray\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import verde as vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = xopr.OPRConnection(cache_dir='radar_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65507af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg_3031 = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\n",
    "coastline = gf.coastline.options(scale='50m').opts(projection=epsg_3031)\n",
    "velocity = rioxarray.open_rasterio(\n",
    "    \"https://its-live-data.s3.amazonaws.com/velocity_mosaic/v2/static/cog/ITS_LIVE_velocity_120m_RGI19A_0000_v02_v.tif\",\n",
    "    chunks='auto', overview_level=4, cache=False\n",
    ").squeeze().drop_vars(['spatial_ref', 'band']).rename('velocity (m/year)')\n",
    "velocity_map = velocity.hvplot.image(x='x', y='y', cmap='gray_r').opts(clim=(0,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb42795",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = xopr.geometry.get_antarctic_regions(name=[\"Vincennes_Bay\", \"Underwood\"], merge_regions=True, simplify_tolerance=100)\n",
    "region_projected = xopr.geometry.project_geojson(region, source_crs='EPSG:4326', target_crs=\"EPSG:3031\")\n",
    "\n",
    "region_hv = hv.Polygons([region_projected]).opts(\n",
    "    color='green',\n",
    "    line_color='black',\n",
    "    fill_alpha=0.3)\n",
    "\n",
    "(velocity_map * coastline * region_hv).opts(aspect='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = opr.query_frames(geometry=region).to_crs('EPSG:3031')\n",
    "print(f\"Found {len(gdf)} radar frames in the selected region.\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde48ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_frames_hv = gdf.hvplot(by='collection', hover_cols=['id'])\n",
    "(velocity_map * coastline * region_hv * radar_frames_hv).opts(aspect='equal', legend_position='top_left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34178f",
   "metadata": {},
   "source": [
    "### Getting bed picks\n",
    "\n",
    "Now that we've got our frames selected, we can load layer information for them. Layer information includes surface and bed and might come from the OPS API or from layerdata files hosted on OPR servers and indexed in the STAC catalog.\n",
    "\n",
    "See https://gitlab.com/openpolarradar/opr/-/wikis/Layer-File-Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ds_list = []\n",
    "\n",
    "with tqdm(gdf.iterrows(), total=len(gdf)) as t:\n",
    "    for id, frame in t:\n",
    "        t.set_description(f\"{id}\")\n",
    "        layers = opr.get_layers(frame)\n",
    "        bed_layer_name = None\n",
    "        if 'standard:bottom' in layers: # Generally, the picked bed should be in group \"standard\" with layer name \"bottom\"\n",
    "            bed_layer_name = 'standard:bottom'\n",
    "        elif ':bottom' in layers:\n",
    "            bed_layer_name = ':bottom' # But occasionally it seems to be missing the group\n",
    "        else:\n",
    "            continue  # No bed layer found\n",
    "        # Layers are stored in terms of two-way travel time to avoid any questions about travel speed within ice\n",
    "        # This is different from how BedMap layers are stored, but it does make more sense when the radar data is availble to use twtt\n",
    "        layer_wgs84 = xopr.radar_util.layer_twtt_to_range(layers[bed_layer_name], layers[\"standard:surface\"], vertical_coordinate='wgs84').rename({'lat': 'Latitude', 'lon': 'Longitude'})\n",
    "        layer_wgs84 = xopr.geometry.project_dataset(layer_wgs84, target_crs='EPSG:3031')\n",
    "        layer_wgs84 = layer_wgs84.dropna('slow_time', subset=['wgs84'])\n",
    "        layer_wgs84['source'] = id\n",
    "        layer_ds_list.append(layer_wgs84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326cee7",
   "metadata": {},
   "source": [
    "We can now combine all of the layers to get a pointwise list of bed picks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce35862",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_merged = xr.concat(layer_ds_list, dim='slow_time')\n",
    "\n",
    "# Just for plots later\n",
    "xlim = (bed_merged.x.min().item(), bed_merged.x.max().item())\n",
    "ylim = (bed_merged.y.min().item(), bed_merged.y.max().item())\n",
    "\n",
    "bed_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1823f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_hv = bed_merged.hvplot.scatter(x='x', y='y', c='wgs84', cmap='turbo', s=2).opts(clabel='Bed Elevation WGS84 (m)')\n",
    "(velocity_map.opts(colorbar=False) * coastline * region_hv * radar_frames_hv * bed_hv).opts(aspect='equal', legend_position='top_left', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513725ef",
   "metadata": {},
   "source": [
    "### Gridding\n",
    "\n",
    "My understanding of what Michael and Mickey want to do is that they want to aggregate the picks onto a regular grid, keeping some summary statistics within each cell. This is pretty ugly here, but just to demonstrate the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_dataarray(d: xr.DataArray, spacing=1000, aggregation_fns={'median': \"median\", 'std': 'std', 'count': \"count\"}):\n",
    "    \"\"\"\n",
    "    Grid a DataArray with x,y coordinates into a regular grid using block aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : xr.DataArray\n",
    "        Input DataArray with 'x' and 'y' coordinates\n",
    "    spacing : float\n",
    "        Grid spacing in the same units as x,y coordinates\n",
    "    aggregation_fns : dict\n",
    "        Dictionary mapping aggregation function names to functions (e.g., {'median': np.median, 'std': np.std})\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset with variables named {d.name}_{fn_name} for each aggregation function\n",
    "    \"\"\"\n",
    "    # Get data extent\n",
    "    x_min = d['x'].min().values\n",
    "    x_max = d['x'].max().values\n",
    "    y_min = d['y'].min().values\n",
    "    y_max = d['y'].max().values\n",
    "    \n",
    "    # Extract coordinate and data values\n",
    "    x_data = d['x'].values\n",
    "    y_data = d['y'].values\n",
    "    data_values = d.values\n",
    "    \n",
    "    # Create grid coordinates\n",
    "    grid_x, grid_y = vd.grid_coordinates(\n",
    "        region=(x_min, x_max, y_min, y_max),\n",
    "        spacing=spacing\n",
    "    )\n",
    "    \n",
    "    # Dictionary to store gridded results for each aggregation function\n",
    "    data_vars = {}\n",
    "    \n",
    "    for fn_name, fn in aggregation_fns.items():\n",
    "        # Use Verde's BlockReduce with the specified aggregation function\n",
    "        gridder = vd.BlockReduce(\n",
    "            reduction=fn, \n",
    "            spacing=spacing, \n",
    "            region=(x_min, x_max, y_min, y_max),\n",
    "            center_coordinates=True\n",
    "        )\n",
    "        block_coords, block_values = gridder.filter(\n",
    "            coordinates=(x_data, y_data), \n",
    "            data=data_values\n",
    "        )\n",
    "        \n",
    "        # Initialize grid with NaN\n",
    "        grid_data = np.full(grid_x.shape, np.nan)\n",
    "        \n",
    "        # Vectorized approach: compute indices directly from coordinates\n",
    "        x_indices = np.floor((block_coords[0] - x_min) / spacing).astype(int)\n",
    "        y_indices = np.floor((block_coords[1] - y_min) / spacing).astype(int)\n",
    "        \n",
    "        for x_idx, y_idx, value in zip(x_indices.flatten(), y_indices.flatten(), block_values.flatten()):\n",
    "            grid_data[y_idx, x_idx] = value\n",
    "        \n",
    "        # Store in dictionary with name pattern\n",
    "        var_name = f\"{d.name}_{fn_name}\" if d.name else f\"data_{fn_name}\"\n",
    "        data_vars[var_name] = (['y', 'x'], grid_data)\n",
    "    \n",
    "    # Create Dataset with all aggregated variables\n",
    "    return xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\n",
    "            'y': grid_y[:, 0],\n",
    "            'x': grid_x[0, :]\n",
    "        }\n",
    "    )\n",
    "\n",
    "gridded = grid_dataarray(bed_merged['wgs84'], spacing=5000)\n",
    "\n",
    "gridded_median_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_median', 'wgs84_std', 'wgs84_count']).opts(\n",
    "    cmap='turbo',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "gridded_std_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_std', 'wgs84_median', 'wgs84_count']).opts(\n",
    "    cmap='inferno',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='Std of WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "(velocity_map * region_hv * coastline * gridded_median_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim) + \\\n",
    "    (velocity_map * region_hv * coastline * gridded_std_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f859",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "OPR and BedMap(1/2/3) are overlapping sets of bed picks, but neither fully encompasses the other. OPR is probably the preferrable source when both have the same bed picks, because OPR facilitaties linking back to the source radar data.\n",
    "\n",
    "(I picked this particular region because there's data from the 2017 UTIG season that we've recently made available through OPR but is missing from BedMap as far as I can tell.)\n",
    "\n",
    "Mathieu has told me that his workflow for resolving discrepancies involves checking the source radar data when it's available to try to confirm which pick is right. It seems pretty high value to maintain the links back to the radar source and make it really easy to load the source data when something needs to be checked out.\n",
    "\n",
    "I think it's important that the same basic workflow can be used to fetch either OPR bed picks, BedMap3 bed picks, or a unified set of both (with conflicts resolved). The question is what should this interface actually look like.\n",
    "\n",
    "It's appealing to me to keep the concept of a STAC catalog that indexes flight paths with data products attached to it with layer picking information. It's not necessarily clear we should follow the OPR standard for what that data product looks like, though. And there might be all-together better ways to deal with all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a882b38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
