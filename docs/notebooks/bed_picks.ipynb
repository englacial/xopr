{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291a2191",
   "metadata": {},
   "source": [
    "---\n",
    "title: Loading bed picks and layer data\n",
    "date: 2025-12-05\n",
    "---\n",
    "\n",
    "For workflows that primarily need surface, bed, or internal layer picks, this notebook demonstrates how to work with OPR layer picking information.\n",
    "\n",
    "This example shows how to load bed picks for a region and grid them onto a regular grid using Verde.\n",
    "\n",
    "(If your primary use case is plotting layers on top of a radargram you've already loaded, the `demo_notebook.ipynb` may be more useful to you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe8678",
   "metadata": {},
   "outputs": [],
   "source": "import xopr\nfrom xopr.bedmap import query_bedmap, query_bedmap_catalog, fetch_bedmap\n\nimport holoviews as hv\nimport xarray as xr\nimport hvplot\nimport hvplot.xarray\nimport hvplot.pandas\nimport geoviews.feature as gf\nimport cartopy.crs as ccrs\nimport rioxarray\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport verde as vd\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import box\nfrom datetime import datetime\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = xopr.OPRConnection(cache_dir='radar_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06d43d",
   "metadata": {},
   "source": [
    "We'll setup some useful backgrounds for context on our maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65507af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg_3031 = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\n",
    "coastline = gf.coastline.options(scale='50m').opts(projection=epsg_3031)\n",
    "velocity = rioxarray.open_rasterio(\n",
    "    \"https://its-live-data.s3.amazonaws.com/velocity_mosaic/v2/static/cog/ITS_LIVE_velocity_120m_RGI19A_0000_v02_v.tif\",\n",
    "    chunks='auto', overview_level=4, cache=False\n",
    ").squeeze().drop_vars(['spatial_ref', 'band']).rename('velocity (m/year)')\n",
    "velocity_map = velocity.hvplot.image(x='x', y='y', cmap='gray_r').opts(clim=(0,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c8df5",
   "metadata": {},
   "source": [
    "For this example, we'll focus on a specific region. Feel free to try swapping this region out for any other, of course.\n",
    "\n",
    "The Vincennes Bay area has some deep troughs that run across flow, making it an interesting area to loop at bed topography. This region is covered more extensively by UTIG data, which has only recently become available through OPR. Keep in mind that not all of this data yet has bed picks, as we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb42795",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = xopr.geometry.get_antarctic_regions(name=[\"Vincennes_Bay\", \"Underwood\"], merge_regions=True, simplify_tolerance=100)\n",
    "region_projected = xopr.geometry.project_geojson(region, source_crs='EPSG:4326', target_crs=\"EPSG:3031\")\n",
    "\n",
    "region_hv = hv.Polygons([region_projected]).opts(\n",
    "    color='green',\n",
    "    line_color='black',\n",
    "    fill_alpha=0.3)\n",
    "\n",
    "(velocity_map * coastline * region_hv).opts(aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c334c",
   "metadata": {},
   "source": [
    "Querying for bed picks starts the same as any other radar query. We'll begin by fetching the STAC items corresponding to the radar data we want. As a reminder, this step doesn't load any actual radar data yet -- we're just getting the paths along which the radar data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = opr.query_frames(geometry=region).to_crs('EPSG:3031')\n",
    "print(f\"Found {len(gdf)} radar frames in the selected region.\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde48ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_frames_hv = gdf.hvplot(by='collection', hover_cols=['id'])\n",
    "(velocity_map * coastline * region_hv * radar_frames_hv).opts(aspect='equal', legend_position='top_left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34178f",
   "metadata": {},
   "source": [
    "### Getting bed picks\n",
    "\n",
    "Now that we've got our frames selected, we can load layer information for them. Layer information includes surface and bed and might come from the OPS API or from layerdata files hosted on OPR servers and indexed in the STAC catalog.\n",
    "\n",
    "See https://gitlab.com/openpolarradar/opr/-/wikis/Layer-File-Guide\n",
    "\n",
    "xOPR tries to abstract away the difference between the layer database and the layer files by formatting both to look like the layer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ds_list = []\n",
    "\n",
    "with tqdm(gdf.iterrows(), total=len(gdf)) as t:\n",
    "    for id, frame in t:\n",
    "        t.set_description(f\"{id}\")\n",
    "        layers = opr.get_layers(frame)\n",
    "        bed_layer_name = None\n",
    "        if 'standard:bottom' in layers: # Generally, the picked bed should be in group \"standard\" with layer name \"bottom\"\n",
    "            bed_layer_name = 'standard:bottom'\n",
    "        elif ':bottom' in layers:\n",
    "            bed_layer_name = ':bottom' # But occasionally it seems to be missing the group\n",
    "        else:\n",
    "            continue  # No bed layer found\n",
    "        # Layers are stored in terms of two-way travel time to avoid any questions about travel speed within ice\n",
    "        # This is different from how BedMap layers are stored, but it does make more sense when the radar data is availble to use twtt\n",
    "        layer_wgs84 = xopr.radar_util.layer_twtt_to_range(layers[bed_layer_name], layers[\"standard:surface\"], vertical_coordinate='wgs84').rename({'lat': 'Latitude', 'lon': 'Longitude'})\n",
    "        layer_wgs84 = xopr.geometry.project_dataset(layer_wgs84, target_crs='EPSG:3031')\n",
    "        layer_wgs84 = layer_wgs84.dropna('slow_time', subset=['wgs84'])\n",
    "        layer_wgs84['source'] = id\n",
    "        layer_ds_list.append(layer_wgs84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326cee7",
   "metadata": {},
   "source": [
    "We can now combine all of the layers to get a pointwise list of bed picks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce35862",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_merged = xr.concat(layer_ds_list, dim='slow_time')\n",
    "\n",
    "# Just for plots later\n",
    "xlim = (bed_merged.x.min().item(), bed_merged.x.max().item())\n",
    "ylim = (bed_merged.y.min().item(), bed_merged.y.max().item())\n",
    "\n",
    "bed_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1823f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_hv = bed_merged.hvplot.scatter(x='x', y='y', c='wgs84', cmap='turbo', s=2).opts(clabel='Bed Elevation WGS84 (m)')\n",
    "(velocity_map.opts(colorbar=False) * coastline * region_hv * radar_frames_hv * bed_hv).opts(aspect='equal', legend_position='top_left', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60423f7f",
   "metadata": {},
   "source": [
    "As you can see, many of the radar lines are (as of the time of writing) missing bed picks.\n",
    "\n",
    "If you're looking at the Vincennes Bay region, you'll see the very deep trough running roughly grid top to bottom in the radar bed picks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513725ef",
   "metadata": {},
   "source": [
    "### Gridding\n",
    "\n",
    "What you want to do with the bed picks is, of course, up to you. One use case might be to aggregate these picks onto a common grid. We'll show an example of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_dataarray(d: xr.DataArray, spacing=1000, aggregation_fns={'median': \"median\", 'std': 'std', 'count': \"count\"}):\n",
    "    \"\"\"\n",
    "    Grid a DataArray with x,y coordinates into a regular grid using block aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : xr.DataArray\n",
    "        Input DataArray with 'x' and 'y' coordinates\n",
    "    spacing : float\n",
    "        Grid spacing in the same units as x,y coordinates\n",
    "    aggregation_fns : dict\n",
    "        Dictionary mapping aggregation function names to functions (e.g., {'median': np.median, 'std': np.std})\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset with variables named {d.name}_{fn_name} for each aggregation function\n",
    "    \"\"\"\n",
    "    # Get data extent\n",
    "    x_min = d['x'].min().values\n",
    "    x_max = d['x'].max().values\n",
    "    y_min = d['y'].min().values\n",
    "    y_max = d['y'].max().values\n",
    "    \n",
    "    # Extract coordinate and data values\n",
    "    x_data = d['x'].values\n",
    "    y_data = d['y'].values\n",
    "    data_values = d.values\n",
    "    \n",
    "    # Create grid coordinates\n",
    "    grid_x, grid_y = vd.grid_coordinates(\n",
    "        region=(x_min, x_max, y_min, y_max),\n",
    "        spacing=spacing\n",
    "    )\n",
    "    \n",
    "    # Dictionary to store gridded results for each aggregation function\n",
    "    data_vars = {}\n",
    "    \n",
    "    for fn_name, fn in aggregation_fns.items():\n",
    "        # Use Verde's BlockReduce with the specified aggregation function\n",
    "        gridder = vd.BlockReduce(\n",
    "            reduction=fn, \n",
    "            spacing=spacing, \n",
    "            region=(x_min, x_max, y_min, y_max),\n",
    "            center_coordinates=True\n",
    "        )\n",
    "        block_coords, block_values = gridder.filter(\n",
    "            coordinates=(x_data, y_data), \n",
    "            data=data_values\n",
    "        )\n",
    "        \n",
    "        # Initialize grid with NaN\n",
    "        grid_data = np.full(grid_x.shape, np.nan)\n",
    "        \n",
    "        # Vectorized approach: compute indices directly from coordinates\n",
    "        x_indices = np.floor((block_coords[0] - x_min) / spacing).astype(int)\n",
    "        y_indices = np.floor((block_coords[1] - y_min) / spacing).astype(int)\n",
    "        \n",
    "        for x_idx, y_idx, value in zip(x_indices.flatten(), y_indices.flatten(), block_values.flatten()):\n",
    "            grid_data[y_idx, x_idx] = value\n",
    "        \n",
    "        # Store in dictionary with name pattern\n",
    "        var_name = f\"{d.name}_{fn_name}\" if d.name else f\"data_{fn_name}\"\n",
    "        data_vars[var_name] = (['y', 'x'], grid_data)\n",
    "    \n",
    "    # Create Dataset with all aggregated variables\n",
    "    return xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\n",
    "            'y': grid_y[:, 0],\n",
    "            'x': grid_x[0, :]\n",
    "        }\n",
    "    )\n",
    "\n",
    "gridded = grid_dataarray(bed_merged['wgs84'], spacing=5000)\n",
    "\n",
    "gridded_median_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_median', 'wgs84_std', 'wgs84_count']).opts(\n",
    "    cmap='turbo',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "gridded_std_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_std', 'wgs84_median', 'wgs84_count']).opts(\n",
    "    cmap='inferno',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='Std of WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "(velocity_map * region_hv * coastline * gridded_median_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim) + \\\n",
    "    (velocity_map * region_hv * coastline * gridded_std_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f859",
   "metadata": {},
   "source": "## Bedmap Data Integration\n\nThe BedMap(1/2/3) datasets contain an enormous catalog of surface and bed picks. BedMap includes data from surveys that aren't yet in the OPR catalog, while OPR has some radar data that hasn't made it into BedMap. xopr provides unified access to bed picks from both sources.\n\nThe bedmap data is hosted on Google Cloud Storage at `gs://opr_stac/bedmap/`. The query process works in two stages:\n1. **STAC Catalog Query**: Find GeoParquet files that intersect with the query geometry/time\n2. **DuckDB Partial Reads**: Fetch only relevant rows from those files using SQL pushdown\n\nThis approach minimizes data transfer - only the data you need is downloaded!"
  },
  {
   "cell_type": "code",
   "id": "b434d62f",
   "metadata": {},
   "source": "# Define a query region (using the same Vincennes Bay area)\n# We'll use the same region we defined above for consistency\nquery_bbox = box(region_projected.bounds[0], region_projected.bounds[1], \n                 region_projected.bounds[2], region_projected.bounds[3])\n\n# Query the catalog to see what bedmap files match our region\nprint(\"Querying STAC catalog for matching bedmap files...\")\n\ncatalog_items = query_bedmap_catalog(\n    geometry=region,  # Use WGS84 geometry\n    collections=['bedmap2', 'bedmap3']\n)\n\nif not catalog_items.empty:\n    print(f\"\\nFound {len(catalog_items)} matching files:\")\n    for _, row in catalog_items.head(10).iterrows():\n        props = row['properties'] if 'properties' in row else {}\n        name = props.get('name', row.get('id', 'unknown'))\n        row_count = props.get('row_count', 0)\n        print(f\"  - {name}: {row_count:,} rows\")\n    if len(catalog_items) > 10:\n        print(f\"  ... and {len(catalog_items) - 10} more\")\nelse:\n    print(\"No matching files found.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "nz6h83qmwt",
   "source": "# Query the actual bedmap data from cloud storage\nprint(\"Querying bedmap data from cloud GeoParquet files...\")\nprint(f\"Query region bounds: {region.bounds}\")\n\nbedmap_df = query_bedmap(\n    geometry=region,\n    collections=['bedmap2', 'bedmap3'],\n    max_rows=10000,  # Limit for demo\n    exclude_geometry=True\n)\n\nif not bedmap_df.empty:\n    print(f\"\\nRetrieved {len(bedmap_df):,} points from bedmap\")\n    print(\"\\nColumns available:\")\n    print(bedmap_df.columns.tolist())\n    print(\"\\nFirst 5 rows:\")\n    display(bedmap_df.head())\nelse:\n    print(\"No bedmap data found in query region.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wdxlz159t6p",
   "source": "# Visualize bedmap data alongside OPR data\nif not bedmap_df.empty and 'lon' in bedmap_df.columns and 'lat' in bedmap_df.columns:\n    # Create GeoDataFrame for bedmap data\n    bedmap_gdf = gpd.GeoDataFrame(\n        bedmap_df,\n        geometry=gpd.points_from_xy(bedmap_df['lon'], bedmap_df['lat']),\n        crs='EPSG:4326'\n    ).to_crs('EPSG:3031')\n    \n    # Find thickness column\n    thickness_col = None\n    for col in bedmap_df.columns:\n        if 'thickness' in col.lower():\n            thickness_col = col\n            break\n    \n    # Plot bedmap data\n    if thickness_col:\n        bedmap_hv = bedmap_gdf.hvplot.points(\n            x='geometry', y='geometry', c=thickness_col,\n            cmap='Blues', s=5, alpha=0.7\n        ).opts(clabel='Ice Thickness (m)')\n    else:\n        bedmap_hv = bedmap_gdf.hvplot.points(x='geometry', color='blue', s=5, alpha=0.7)\n    \n    # Combine with OPR data and map\n    (velocity_map.opts(colorbar=False) * coastline * region_hv * \n     radar_frames_hv * bedmap_hv).opts(\n        aspect='equal', legend_position='top_left', xlim=xlim, ylim=ylim,\n        title='Bedmap data points in Vincennes Bay region'\n    )\nelse:\n    print(\"No bedmap data to visualize.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jpfzhcnvzc",
   "source": "### Temporal Filtering\n\nYou can also filter bedmap data by date range to find data from specific time periods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7bx6dh71256",
   "source": "# Query with both spatial and temporal filters\ntemporal_result = query_bedmap(\n    geometry=region,\n    date_range=(datetime(1994, 1, 1), datetime(2010, 12, 31)),\n    collections=['bedmap2'],\n    max_rows=5000,\n)\n\nif not temporal_result.empty:\n    print(f\"Retrieved {len(temporal_result):,} points from 1994-2010\")\n    \n    # Show unique source files\n    if 'source_file' in temporal_result.columns:\n        print(f\"\\nSource files:\")\n        for src in temporal_result['source_file'].unique()[:10]:\n            count = (temporal_result['source_file'] == src).sum()\n            print(f\"  - {src}: {count:,} points\")\nelse:\n    print(\"No data found for the specified query.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6az65gftf12",
   "source": "### Cloud vs Local Cache Performance\n\nFor repeated queries or large datasets, you can use the `local_cache` option to download the GeoParquet files once and query them locally. This provides significant speedups for subsequent queries.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kekorml4ee",
   "source": "# First, time a cloud-based query (no local cache)\nprint(\"Timing cloud-based query...\")\nt0 = time.time()\ncloud_result = query_bedmap(\n    geometry=region,\n    collections=['bedmap2'],\n    max_rows=50000,\n    exclude_geometry=True\n)\ncloud_time = time.time() - t0\nprint(f\"Cloud query: {cloud_time:.2f}s for {len(cloud_result):,} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "n9am97j2b6",
   "source": "# Download bedmap data to local cache for faster subsequent queries\n# This only needs to be done once - files are cached locally\nprint(\"Fetching bedmap data to local cache...\")\nlocal_paths = fetch_bedmap(\n    geometry=region,\n    collections=['bedmap2'],\n)\nprint(f\"Cached {len(local_paths)} files locally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "67yfqoghy2x",
   "source": "# Now time the same query using local cache\nprint(\"Timing local cache query...\")\nt0 = time.time()\nlocal_result = query_bedmap(\n    geometry=region,\n    collections=['bedmap2'],\n    max_rows=50000,\n    exclude_geometry=True,\n    local_cache=True  # Use locally cached files\n)\nlocal_time = time.time() - t0\nprint(f\"Local query: {local_time:.2f}s for {len(local_result):,} rows\")\n\n# Compare performance\nprint(f\"\\nSpeedup: {cloud_time / local_time:.1f}x faster with local cache\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}