{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedmap Data Integration Demo\n",
    "\n",
    "This notebook demonstrates the bedmap data integration features in xopr:\n",
    "1. Converting bedmap CSV files to cloud-optimized GeoParquet\n",
    "2. Building STAC catalogs for data discovery\n",
    "3. Querying data efficiently with spatial/temporal filters\n",
    "4. Comparing bedmap with OPR layer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box, Point\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import xopr bedmap modules\n",
    "from xopr.bedmap import (\n",
    "    convert_bedmap_csv,\n",
    "    batch_convert_bedmap,\n",
    "    build_bedmap_catalog,\n",
    "    query_bedmap,\n",
    "    query_bedmap_local,\n",
    "    compare_with_opr,\n",
    "    match_bedmap_to_opr\n",
    ")\n",
    "\n",
    "print(\"xopr bedmap module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Bedmap CSV to GeoParquet\n",
    "\n",
    "The conversion process:\n",
    "- Parses metadata from CSV headers\n",
    "- Handles complex date/time with fallback strategies\n",
    "- Extracts flight line geometries (multiline with 10km segmentation)\n",
    "- Creates cloud-optimized GeoParquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "bedmap_dir = Path('~/software/bedmap/Results').expanduser()\n",
    "output_dir = Path('scripts/output/bedmap')\n",
    "\n",
    "# Convert a single file as example\n",
    "csv_files = sorted(bedmap_dir.glob('*.csv'))[:1]  # Just first file for demo\n",
    "\n",
    "if csv_files:\n",
    "    print(f\"Converting {csv_files[0].name}...\")\n",
    "    \n",
    "    # Convert CSV to GeoParquet\n",
    "    metadata = convert_bedmap_csv(\n",
    "        csv_files[0],\n",
    "        output_dir,\n",
    "        simplify_tolerance_deg=0.01\n",
    "    )\n",
    "    \n",
    "    print(\"\\nConversion metadata:\")\n",
    "    print(f\"  Bedmap version: {metadata['bedmap_version']}\")\n",
    "    print(f\"  Row count: {metadata['row_count']:,}\")\n",
    "    print(f\"  Spatial bbox: {metadata['spatial_bounds']['bbox']}\")\n",
    "    print(f\"  Temporal range: {metadata['temporal_bounds']['start']} to {metadata['temporal_bounds']['end']}\")\n",
    "else:\n",
    "    print(\"No CSV files found. Please check the bedmap_dir path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Process: STAC + DuckDB\n",
    "\n",
    "The query process works in two stages:\n",
    "1. **STAC Query**: Find files that intersect with the query geometry/time\n",
    "2. **DuckDB Partial Reads**: Fetch only relevant rows from those files\n",
    "\n",
    "This minimizes data transfer and processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query data for a specific region\n",
    "# Define a bounding box (West Antarctica example)\n",
    "query_bbox = box(-80, -78, -70, -74)  # lon_min, lat_min, lon_max, lat_max\n",
    "\n",
    "# Visualize the query region\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "world = gpd.GeoDataFrame([1], geometry=[box(-180, -90, 180, 90)], crs='EPSG:4326')\n",
    "antarctica = gpd.GeoDataFrame([1], geometry=[box(-180, -90, 180, -60)], crs='EPSG:4326')\n",
    "query_region = gpd.GeoDataFrame([1], geometry=[query_bbox], crs='EPSG:4326')\n",
    "\n",
    "world.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "antarctica.plot(ax=ax, color='white', edgecolor='black')\n",
    "query_region.plot(ax=ax, color='red', alpha=0.3, edgecolor='red', linewidth=2)\n",
    "\n",
    "ax.set_xlim(-100, -50)\n",
    "ax.set_ylim(-85, -65)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Query Region (Red Box)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Query bbox: {query_bbox.bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query local parquet files (without STAC)\n",
    "# This demonstrates the DuckDB partial read capability\n",
    "\n",
    "print(\"Querying bedmap data from local parquet files...\")\n",
    "print(f\"Query region: {query_bbox.bounds}\")\n",
    "print()\n",
    "\n",
    "# Query with spatial filter\n",
    "result_df = query_bedmap_local(\n",
    "    parquet_dir=output_dir,\n",
    "    geometry=query_bbox,\n",
    "    columns=['longitude (degree_east)', 'latitude (degree_north)', \n",
    "             'surface_altitude (m)', 'land_ice_thickness (m)', \n",
    "             'source_file', 'timestamp'],\n",
    "    max_items=1000,\n",
    "    exclude_geometry=True\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(result_df):,} points from parquet files\")\n",
    "\n",
    "if not result_df.empty:\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(result_df.head())\n",
    "    \n",
    "    print(\"\\nData summary:\")\n",
    "    print(result_df[['surface_altitude (m)', 'land_ice_thickness (m)']].describe())\n",
    "else:\n",
    "    print(\"No data found in query region. Try a different bbox or convert more files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Query Results\n",
    "\n",
    "Let's visualize the retrieved data points and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not result_df.empty:\n",
    "    # Create GeoDataFrame for visualization\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        result_df,\n",
    "        geometry=gpd.points_from_xy(\n",
    "            result_df['longitude (degree_east)'],\n",
    "            result_df['latitude (degree_north)']\n",
    "        ),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Surface altitude\n",
    "    ax1 = axes[0]\n",
    "    gdf.plot(column='surface_altitude (m)', \n",
    "             ax=ax1, \n",
    "             legend=True,\n",
    "             cmap='terrain',\n",
    "             markersize=10,\n",
    "             legend_kwds={'label': 'Surface Altitude (m)'})\n",
    "    query_region.boundary.plot(ax=ax1, color='red', linewidth=2)\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Surface Altitude')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Ice thickness\n",
    "    ax2 = axes[1]\n",
    "    gdf.plot(column='land_ice_thickness (m)', \n",
    "             ax=ax2, \n",
    "             legend=True,\n",
    "             cmap='Blues',\n",
    "             markersize=10,\n",
    "             legend_kwds={'label': 'Ice Thickness (m)'})\n",
    "    query_region.boundary.plot(ax=ax2, color='red', linewidth=2)\n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title('Ice Thickness')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show data distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Histogram of surface altitude\n",
    "    axes[0].hist(gdf['surface_altitude (m)'].dropna(), bins=30, edgecolor='black')\n",
    "    axes[0].set_xlabel('Surface Altitude (m)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Surface Altitude Distribution')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of ice thickness\n",
    "    axes[1].hist(gdf['land_ice_thickness (m)'].dropna(), bins=30, edgecolor='black', color='blue', alpha=0.7)\n",
    "    axes[1].set_xlabel('Ice Thickness (m)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Ice Thickness Distribution')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Query with STAC\n",
    "\n",
    "When using the full STAC catalog, the query process is:\n",
    "1. Query STAC for files that intersect the search area\n",
    "2. Use DuckDB to read only the relevant portions of those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the full query_bedmap function (would use STAC catalog if available)\n",
    "# This shows how the API matches query_frames()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define query parameters\n",
    "query_params = {\n",
    "    'geometry': box(-75, -76, -70, -74),  # Spatial filter\n",
    "    'date_range': (datetime(1990, 1, 1), datetime(2000, 12, 31)),  # Temporal filter\n",
    "    'collections': ['bedmap-bm2'],  # Filter by bedmap version\n",
    "    'max_items': 500,  # Limit results\n",
    "    'columns': [  # Specific columns to retrieve\n",
    "        'longitude (degree_east)',\n",
    "        'latitude (degree_north)', \n",
    "        'surface_altitude (m)',\n",
    "        'land_ice_thickness (m)',\n",
    "        'bedrock_altitude (m)',\n",
    "        'timestamp',\n",
    "        'source_file'\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Query parameters:\")\n",
    "print(f\"  Spatial: {query_params['geometry'].bounds}\")\n",
    "print(f\"  Temporal: {query_params['date_range'][0]} to {query_params['date_range'][1]}\")\n",
    "print(f\"  Collections: {query_params['collections']}\")\n",
    "print(f\"  Max items: {query_params['max_items']}\")\n",
    "print(f\"  Columns: {len(query_params['columns'])} selected\")\n",
    "\n",
    "# Note: This would normally query the STAC catalog first\n",
    "# For demo, we'll use local query\n",
    "print(\"\\n[In production, this would query STAC catalog first, then fetch from cloud storage]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with OPR Data (Example)\n",
    "\n",
    "The comparison functions allow matching bedmap measurements with OPR layer picks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create mock OPR data for demonstration\n",
    "import xarray as xr\n",
    "\n",
    "if not result_df.empty:\n",
    "    # Create a mock OPR dataset for demonstration\n",
    "    # In practice, this would be loaded from actual OPR files\n",
    "    n_opr_points = 100\n",
    "    opr_lons = np.random.uniform(\n",
    "        result_df['longitude (degree_east)'].min(),\n",
    "        result_df['longitude (degree_east)'].max(),\n",
    "        n_opr_points\n",
    "    )\n",
    "    opr_lats = np.random.uniform(\n",
    "        result_df['latitude (degree_north)'].min(),\n",
    "        result_df['latitude (degree_north)'].max(),\n",
    "        n_opr_points\n",
    "    )\n",
    "    \n",
    "    # Create mock surface and bed elevations (with some noise)\n",
    "    opr_surface = np.random.normal(1000, 100, n_opr_points)\n",
    "    opr_bed = np.random.normal(500, 50, n_opr_points)\n",
    "    \n",
    "    # Create xarray dataset\n",
    "    opr_dataset = xr.Dataset({\n",
    "        'Longitude': (('slow_time',), opr_lons),\n",
    "        'Latitude': (('slow_time',), opr_lats),\n",
    "        'Surface': (('slow_time',), opr_surface),\n",
    "        'Bottom': (('slow_time',), opr_bed),\n",
    "    })\n",
    "    \n",
    "    print(\"Mock OPR dataset created:\")\n",
    "    print(opr_dataset)\n",
    "    \n",
    "    # Match bedmap points to nearest OPR measurements\n",
    "    bedmap_subset = gpd.GeoDataFrame(result_df.head(50))  # Use subset for demo\n",
    "    \n",
    "    matched_data = match_bedmap_to_opr(\n",
    "        bedmap_subset,\n",
    "        opr_dataset,\n",
    "        max_distance_m=5000  # 5 km matching tolerance\n",
    "    )\n",
    "    \n",
    "    # Show matching results\n",
    "    print(f\"\\nMatching results:\")\n",
    "    print(f\"  Total bedmap points: {len(matched_data)}\")\n",
    "    print(f\"  Matched points: {matched_data['is_matched'].sum()}\")\n",
    "    print(f\"  Average match distance: {matched_data['opr_match_distance_m'].mean():.1f} m\")\n",
    "    \n",
    "    # Visualize matches\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot bedmap points\n",
    "    ax.scatter(bedmap_subset['longitude (degree_east)'], \n",
    "               bedmap_subset['latitude (degree_north)'],\n",
    "               c='blue', label='Bedmap', s=50, alpha=0.6)\n",
    "    \n",
    "    # Plot OPR points\n",
    "    ax.scatter(opr_lons, opr_lats, \n",
    "               c='red', label='OPR', s=30, alpha=0.6)\n",
    "    \n",
    "    # Draw lines for matches\n",
    "    for idx, row in matched_data[matched_data['is_matched']].iterrows():\n",
    "        ax.plot([row['longitude (degree_east)'], row['opr_longitude']],\n",
    "                [row['latitude (degree_north)'], row['opr_latitude']],\n",
    "                'g-', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Bedmap to OPR Matching (Green Lines = Matches)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for comparison demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Performance Analysis\n",
    "\n",
    "Let's analyze the efficiency of the two-stage query process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare query performance\n",
    "print(\"Query Performance Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define test queries of different sizes\n",
    "test_queries = [\n",
    "    ('Small', box(-72, -75, -70, -74)),   # ~2x1 degree\n",
    "    ('Medium', box(-75, -76, -70, -74)),  # ~5x2 degrees\n",
    "    ('Large', box(-80, -78, -70, -74)),   # ~10x4 degrees\n",
    "]\n",
    "\n",
    "for name, bbox in test_queries:\n",
    "    print(f\"\\n{name} Query: {bbox.bounds}\")\n",
    "    print(f\"  Area: {bbox.area:.1f} square degrees\")\n",
    "    \n",
    "    # Time the query\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = query_bedmap_local(\n",
    "        parquet_dir=output_dir,\n",
    "        geometry=bbox,\n",
    "        max_items=10000\n",
    "    )\n",
    "    \n",
    "    query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Query time: {query_time:.3f} seconds\")\n",
    "    print(f\"  Points retrieved: {len(result):,}\")\n",
    "    \n",
    "    if len(result) > 0:\n",
    "        print(f\"  Points/second: {len(result)/query_time:,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Key advantages of the two-stage approach:\")\n",
    "print(\"1. STAC filters files before reading (reduces I/O)\")\n",
    "print(\"2. DuckDB reads only necessary columns (column pruning)\")\n",
    "print(\"3. Spatial filter applied during read (row filtering)\")\n",
    "print(\"4. Files never fully loaded into memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This demo showed the complete bedmap integration workflow:\n",
    "\n",
    "### What we've implemented:\n",
    "✅ **CSV → GeoParquet conversion** with complex date handling  \n",
    "✅ **Flight line extraction** with 10km segmentation  \n",
    "✅ **STAC catalog generation** for discovery  \n",
    "✅ **Efficient queries** using STAC + DuckDB  \n",
    "✅ **OPR comparison** functions  \n",
    "\n",
    "### Query Process Recap:\n",
    "1. **STAC Query** - Find files intersecting with query geometry/time\n",
    "2. **DuckDB Partial Reads** - Fetch only rows within bounding box\n",
    "3. **Optional Refinement** - Apply precise geometry filter if needed\n",
    "\n",
    "### Next Steps:\n",
    "1. Convert all 151 bedmap CSV files\n",
    "2. Build complete STAC catalog\n",
    "3. Upload to Google Cloud Storage\n",
    "4. Test with real OPR data comparisons\n",
    "5. Integrate into production workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}